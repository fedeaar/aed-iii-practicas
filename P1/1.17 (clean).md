### Dado un conjunto de actividades $A = \{A_1\ ...\ A_n\}$, el problema de selección de actividades consiste en encontrar un subconjunto de actividades $S$ de cardinalidad máxima, tal que ningún par de actividades de $S$ se solapen en el tiempo. Cada actividad $A_i$ se realiza en algún intervalo de tiempo $(s_i,\ t_i)$, siendo $s_i \in \mathbb{N}$ su momento inicial y $t_i \in \mathbb{N}$ su momento final. Suponemos que $1 \leq s_i < t_i \leq 2n$ para todo $1 \leq i \leq n$.

<br>

### (a) Considerar la siguiente analogía con el problema de la fiesta: cada posible actividad es un invitado y dos actividades pueden "invitarse" a la fiesta cuando no se solapan en el tiempo. A partir de esta analogía, proponga un algoritmo de backtracking para resolver el problema de selección de actividades. ¿Cuál es la complejidad del algoritmo?

\
Podemos considerar el siguiente algoritmo, muy similar al propuesto para la parte (b) del problema de la fiesta (Ejercicio 1.16).

```
proc act(S, A):
    si |A| = 0:
        retornar S
    si no:
        elegir actividad de A
        A' <- remover_incompatibles(A, a)
        A  <- act(S + a, A')
        B  <- act(S, A\a)
        si |A| >= |B|:
            retornar A
        si no:
            retornar B
```
El problema se resuelve a partir de la llamada $\text{act}(\emptyset,\ A)$.

Dado que se realizan dos llamadas recursivas sobre un árbol de backtracking de tamaño $O(2^n)$, correspondiente a todos los subconjuntos posibles de $n$ actividades, y la función $\text{remover\_incompatibles}$ requiere buscar en un arreglo desordenado de tamaño $O(n)$, sigue que la complejidad temporal de peor caso es $O(n2^n)$.

$\blacksquare$


<br>

### (b) Supongamos que $A$ está ordenado por orden de comienzo de la actividad, i.e., $s_i \leq s_{i+1}$ para todo $1 \leq i < n$. Escribir una función recursiva $\text{act}(A,\ S,\ i)$ que encuentre el conjunto máximo de actividades seleccionables que contenga a $S \subset \{A_1\ . . .\ A_{i−1}\}$ y que se obtenga agregando únicamente actividades de $\{A_i\ . . .\ A_n\}$. Para reflexionar: ¿por qué se puede definir $\text{act}$ en este caso y no en el inciso (e) del Ejercicio 16?

\
Podemos considerar la siguiente función.

$\begin{align}\nonumber
    \text{act}(A,\ S,\ i) = 
        \begin{cases}
            S & i > n \\
            \text{act}(A,\ S\oplus A_i,\ k) & i \leq n\ \wedge\ a \geq b \\
            \text{act}(A,\ S,\ i+1) &i \leq n\ \wedge\ a < b
        \end{cases}
\end{align}$
donde $k > i$ es el mínimo natural que satisface que el comienzo de la actividad $k$ es mayor o igual al final de la actividad $i$, $a = |\text{act}(A, S\oplus V_i,\ k)|$ y $b = |\text{act}(A, S, i+1)|$.

Podemos definir $\text{act}(A, S, i)$, en este caso, ya que el ordenamiento impuesto para las actividades define también un ordenamiento de las actividades en conflicto con la actividad $i$. Luego, basta con encontrar la primer actividad no en conflicto para saber cuál es el subconjunto a considerar en la próxima recursión.

$\blacksquare$


<br>

### (c) Implementar un algoritmo de programación dinámica para el problema de selección de actividades que se base en la función del inciso (b). ¿Cuál es su complejidad temporal y cuál es el espacio extra requerido?

\
Para esta versión, considerémos un problema cercano: encontrar el tamaño de la solución máxima con programación dinámica. El algoritmo siguiente presenta una solución top-down.

```
M[i][j] <- ⊥ para todo 0 <= i < n y 1 <= j <= n
proc act(s, i):
    si i > n:
        retornar s
    si M[s, i] = ⊥:
        a <- act(s, i+1)
        k <- i + 1
        mientras k <= n y comienzo(A[k]) < fin(A[i]):
            k <- k + 1
        b <- act(s+1, k)
        M[s, i] <- max(a, b)
    retornar M[s, i]
```
Luego, podemos reconstruir una solución máxima a partir de los datos de la matriz $M$. Para ello, notar que la primer fila de $M$ ($s = 0$) tiene, en la $j$-ésima columna, $1 \leq j \leq n$, el valor de la solución máxima para las últimas $n+1-j$ actividades. Entonces, necesariamente, $M_{0n} = 1$ y, si una solución óptima no incluye a $j$, $M_{0j} = M_{0j+1}$. 

El siguiente algoritmo propone una forma de aprovechar esta estructura. 

```
    act(0, 1)
    S <- conjunto vacio
    s <- 0
    para j en n ... 1:  
        t <- M[1][j]
        si t > s:
            s <- t
            push(S, A[j])
    retornar S
```

Vemos que el primer algoritmo tiene $O(n^2)$ instancias para las cuales realiza un ciclo con costo en $O(n)$, mientras que la reconstrucción de una solución tiene un costo en $O(n)$. Luego, la complejidad de peor caso es $O(n^3)$.

Dado que utilizamos una matriz de tamaño $O(n^2)$ y un conjunto de tamaño $O(n)$, sigue que la complejidad espacial auxiliar es $O(n^2)$.

$\blacksquare$


<br>

### (d) Considerar la siguiente estrategia golosa para resolver el problema de selección de actividades: elegir la actividad cuyo momento final sea lo más temprano posible, de entre todas las actividades que no se solapen con las actividades ya elegidas. Demostrar que un algoritmo goloso que implementa la estrategia anterior es correcto. Ayuda: demostrar por inducción que la solución parcial $B_1\ . . .\ B_i$ que brinda el algoritmo goloso en el paso $i$ se puede extender a una solución óptima. Para ello, suponga en el paso inductivo que $B_1\ . . .\ B_i,\ B_{i+1}$ es la solución golosa y que $B_1\ . . .\ B_i,\ C_{i+1}\ . . .\ C_j$ es la extensión óptima que existe por inducción y muestre que $B_1\ . . .\ B_{i+1},\ C_{i+2}\ . . .\ C_j$ es una extensión óptima de $B_1\ . . .\ B_{i+1}$.

\
Para el caso base, $i = 0$, el algoritmo todavía no eligió ninguna actividad. Luego, podemos extender la solución a una solución optima de manera trivial.

Supongamos ahora, para $i > 0$, que tenemos una solución parcial $B_1\ ...\ B_i$ de actividades elegidas por nuestro algoritmo que se puede extender a una solución óptima
$\begin{align}\nonumber
    B_1\ ...\ B_i,\ C_{i+1}\ ...\ C_j
\end{align}$ 
donde, sin pérdida de generalidad, vamos a suponer que la secuencia de actividades sin solapamiento $C_{i+1}\ ...\ C_j$ está ordenada por tiempo de finalización. 

Notar que, por nuestro método de selección, $B_1\ ...\ B_i$ también debe estar ordenado de la misma manera. Luego, todas las actividades $C_{i+1}\ ...\ C_j$ deben empezar después de que termine $B_i$ para que la extensión sea válida. Si no, habría solapamientos, ya que, por definición de la estrategia, deben terminar después que $B_i$.

Consideremos ahora la solución parcial golosa $B_1\ ...\ B_{i+1}$, donde $B_{i+1}$ es la actividad cuyo momento final ocurre antes entre todas las actividades restantes y no se solapa con las actividades ya seleccionadas. 

Como $B_{i+1}$ termina antes que $C_{i+1}$ o $B_{i+1} = C_{i+1}$, entonces $B_{i+1}$ no se solapa con ninguna actividad $C_{i+2}\ ...\ C_j$. Esto se debe a que $C_{i+1}$ no lo hacía y, por hipótesis inductiva, es la actividad que termina antes en la extensión. Luego, $B_1\ ...\ B_{i+1}$ se puede extender por reemplazo directo a la solución óptima 
$\begin{align}\nonumber
    B_1\ ...\ B_{i+1},\ C_{i+2}\ ...\ C_j
\end{align}$

$\blacksquare$


<br>

### (e) Mostrar una implementación del algoritmo cuya complejidad temporal sea $O(n)$.

\
Podemos considerar el siguiente algoritmo, teniendo en cuenta que el tiempo de finalización de las actividades está acotado por $2n$.

```
proc actividades(A):
    ordenar A por tiempo de finalización con counting sort
    P <- conjunto vacio
    i <- 0
    para (s, t) en A:
        si s >= i:
            push(P, (s, t))
            i <- t
    retornar P
```
$\blacksquare$
